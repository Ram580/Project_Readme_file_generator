{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7: Build a Crew to Tailor Job Applications\n",
    "\n",
    "In this lesson, you will built your first multi-agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries are already installed in the classroom. If you're running this notebook on your own machine, you can install the following:\n",
    "```Python\n",
    "!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import libraries, APIs and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- The video uses `gpt-4-turbo`, but due to certain constraints, and in order to offer this course for free to everyone, the code you'll run here will use `gpt-3.5-turbo`.\n",
    "- You can use `gpt-4-turbo` when you run the notebook _locally_ (using `gpt-4-turbo` will not work on the platform)\n",
    "- Thank you for your understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from utils import get_openai_api_key, get_serper_api_key\n",
    "\n",
    "# openai_api_key = get_openai_api_key()\n",
    "# os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-3.5-turbo'\n",
    "# os.environ[\"SERPER_API_KEY\"] = get_serper_api_key()\n",
    "# Setup the Gemini pro LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0, verbose=True, google_api_key=os.environ[\"GOOGLE_API_KEY\"],convert_system_message_to_human=True)\n",
    "\n",
    "#setup the serper api key\n",
    "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"serper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crewAI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with open('inputs\\Ram_Resume_2024.pdf', 'r', encoding='latin-1') as infile:\n",
    "    content = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "  FileReadTool,\n",
    "  ScrapeWebsiteTool,\n",
    "  MDXSearchTool,\n",
    "  PDFSearchTool,\n",
    "  SerperDevTool\n",
    ")\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "read_resume = FileReadTool(file_path='inputs\\Ram_Resume_2024.pdf')\n",
    "semantic_search_resume = PDFSearchTool(mdx=r\"inputs\\Ram_Resume_2024.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncomment and run the cell below if you wish to view `fake_resume.md` in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"inputs\\Ram_Resume_2024.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f26095e110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "# display(Markdown(\"./fake_resume.md\"))\n",
    "from IPython.display import IFrame\n",
    "IFrame('inputs\\Ram_Resume_2024.pdf', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "researcher = Agent(\n",
    "    role=\"Tech Job Researcher\",\n",
    "    goal=\"Make sure to do amazing analysis on \"\n",
    "         \"job posting to help job applicants\",\n",
    "    tools = [scrape_tool, search_tool],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"As a Job Researcher, your prowess in \"\n",
    "        \"navigating and extracting critical \"\n",
    "        \"information from job postings is unmatched.\"\n",
    "        \"Your skills help pinpoint the necessary \"\n",
    "        \"qualifications and skills sought \"\n",
    "        \"by employers, forming the foundation for \"\n",
    "        \"effective application tailoring.\"\n",
    "    ),\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Agent 2: Profiler\n",
    "profiler = Agent(\n",
    "    role=\"Personal Profiler for Engineers\",\n",
    "    goal=\"Do increditble research on job applicants \"\n",
    "         \"to help them stand out in the job market\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Equipped with analytical prowess, you dissect \"\n",
    "        \"and synthesize information \"\n",
    "        \"from diverse sources to craft comprehensive \"\n",
    "        \"personal and professional profiles, laying the \"\n",
    "        \"groundwork for personalized resume enhancements.\"\n",
    "    ),\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# Agent 3: Resume Strategist\n",
    "resume_strategist = Agent(\n",
    "    role=\"Resume Strategist for Engineers\",\n",
    "    goal=\"Find all the best ways to make a \"\n",
    "         \"resume stand out in the job market.\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"With a strategic mind and an eye for detail, you \"\n",
    "        \"excel at refining resumes to highlight the most \"\n",
    "        \"relevant skills and experiences, ensuring they \"\n",
    "        \"resonate perfectly with the job's requirements.\"\n",
    "    ),\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Agent 4: Interview Preparer\n",
    "interview_preparer = Agent(\n",
    "    role=\"Engineering Interview Preparer\",\n",
    "    goal=\"Create interview questions and talking points \"\n",
    "         \"based on the resume and job requirements\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Your role is crucial in anticipating the dynamics of \"\n",
    "        \"interviews. With your ability to formulate key questions \"\n",
    "        \"and talking points, you prepare candidates for success, \"\n",
    "        \"ensuring they can confidently address all aspects of the \"\n",
    "        \"job they are applying for.\"\n",
    "    ),\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# Task for Researcher Agent: Extract Job Requirements\n",
    "research_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the job posting URL provided ({job_posting_url}) \"\n",
    "        \"to extract key skills, experiences, and qualifications \"\n",
    "        \"required. Use the tools to gather content and identify \"\n",
    "        \"and categorize the requirements.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A structured list of job requirements, including necessary \"\n",
    "        \"skills, qualifications, and experiences.\"\n",
    "    ),\n",
    "    agent=researcher,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Task for Profiler Agent: Compile Comprehensive Profile\n",
    "profile_task = Task(\n",
    "    description=(\n",
    "        \"Compile a detailed personal and professional profile \"\n",
    "        \"using the GitHub ({github_url}) URLs, and personal write-up \"\n",
    "        \"({personal_writeup}). Utilize tools to extract and \"\n",
    "        \"synthesize information from these sources.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A comprehensive profile document that includes skills, \"\n",
    "        \"project experiences, contributions, interests, and \"\n",
    "        \"communication style.\"\n",
    "    ),\n",
    "    agent=profiler,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can pass a list of tasks as `context` to a task.\n",
    "- The task then takes into account the output of those tasks in its execution.\n",
    "- The task will not run until it has the output(s) from those tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "# Task for Resume Strategist Agent: Align Resume with Job Requirements\n",
    "resume_strategy_task = Task(\n",
    "    description=(\n",
    "        \"Using the profile and job requirements obtained from \"\n",
    "        \"previous tasks, tailor the resume to highlight the most \"\n",
    "        \"relevant areas. Employ tools to adjust and enhance the \"\n",
    "        \"resume content. Make sure this is the best resume even but \"\n",
    "        \"don't make up any information. Update every section, \"\n",
    "        \"inlcuding the initial summary, work experience, skills, \"\n",
    "        \"and education. All to better reflrect the candidates \"\n",
    "        \"abilities and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"An updated resume that effectively highlights the candidate's \"\n",
    "        \"qualifications and experiences relevant to the job.\"\n",
    "    ),\n",
    "    output_file=\"tailored_resume.md\",\n",
    "    context=[research_task, profile_task],\n",
    "    agent=resume_strategist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "# Task for Interview Preparer Agent: Develop Interview Materials\n",
    "interview_preparation_task = Task(\n",
    "    description=(\n",
    "        \"Create a set of potential interview questions and talking \"\n",
    "        \"points based on the tailored resume and job requirements. \"\n",
    "        \"Utilize tools to generate relevant questions and discussion \"\n",
    "        \"points. Make sure to use these question and talking points to \"\n",
    "        \"help the candiadte highlight the main points of the resume \"\n",
    "        \"and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A document containing key questions and talking points \"\n",
    "        \"that the candidate should prepare for the initial interview.\"\n",
    "    ),\n",
    "    output_file=\"interview_materials.md\",\n",
    "    context=[research_task, profile_task, resume_strategy_task],\n",
    "    agent=interview_preparer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "job_application_crew = Crew(\n",
    "    agents=[researcher,\n",
    "            profiler,\n",
    "            resume_strategist,\n",
    "            interview_preparer],\n",
    "\n",
    "    tasks=[research_task,\n",
    "           profile_task,\n",
    "           resume_strategy_task,\n",
    "           interview_preparation_task],\n",
    "\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Crew\n",
    "\n",
    "- Set the inputs for the execution of the crew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "job_application_inputs = {\n",
    "    'job_posting_url': 'https://www.google.com/search?q=IBM+Generative+AI+developer+job&oq=IBM+Generative+AI+developer+job&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQLhhA0gEJMTM0NzFqMGoxqAIAsAIA&sourceid=chrome&ie=UTF-8&ibp=htl;jobs&htidocid=QW-A1gKy_AIkmQGNAAAAAA%3D%3D&sa=X&ved=2ahUKEwik3qaz57WGAxXmk1YBHbjXAHkQkd0GegQIERAB#fpstate=tldetail&htivrt=jobs&htiq=IBM+Generative+AI+developer+job&htidocid=QW-A1gKy_AIkmQGNAAAAAA%3D%3D&sxsrf=ADLYWIIMiqTXv_35TSLVUbF9aCG8-yLomg:1717086961911',\n",
    "    'github_url': 'https://github.com/Ram580',\n",
    "    'personal_writeup': \"\"\"Highly motivated and results-oriented Data Analyst with 2.5 years of experience driving data-driven decision making through advanced analytics, ML, Deep learning, NLP and Generative AI solutions.\n",
    "    adept at various domains – marketing analytics , manufacturing, pharma and airlines.  and expert in multiple\n",
    "    programming languages and frameworks.His Experties are Advanced Analytics : Utilized Machine learning(Regression, Classification, Time series forecasting), Deep Learning (LSTM), NLP, and GenAI (Langchain) to solve complex business problems in Pharma and Healthcare.\n",
    "ETL Pipeline Development: Built and automated efficient ETL pipelines  leveraging Python, SQL, PySpark that reduced data turnaround time by 30%.\n",
    "Generative AI: Expertise in LLM fine-tuning (PEFT), RLHF, prompt engineering for various AI models, RAG Applications, Multimodal AI applications development, evaluation and deployment,\n",
    "Actionable Insights & Dashboards: Created high-performance dashboards (Power BI, Tableau) to empower data-driven decision making (e.g., patient churn reduction).\n",
    "Forecasting & Modeling: Built, finetuned and enhanced multiple forecasting models for various use cases like regression, classification, time series forecasting etc.,\n",
    "Project Leadership: Mentored data science interns and led junior analysts in various data science projects (EDA to deployment).\n",
    "Cross-Industry Experience: Possesses experience in diverse sectors (Pharma, Healthcare, Airlines).\n",
    "Project Versatility: Implemented projects ranging from prescriber choice model for measuring campaign effectiveness in pharma, airline resource allocation (passenger footfall forecasting), Gen AI knowledge repositories chatbot (Langchain, LLMs).\n",
    "Cloud and Deployment Skills: Utilized tools like Docker, Git, Github Actions and AWS Sagemaker, Amazon Bedrock, S3, AWS Lambda, API Gateway, AWS RDS for project deployments\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: LLMs can provide different outputs for they same input, so what you get might be different than what you see in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "### this execution will take a few minutes to run\n",
    "result = job_application_crew.kickoff(inputs=job_application_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dislplay the generated `tailored_resume.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RAM BADANA\n",
       "Data Scientist\n",
       "Email: ramhemanth580@gmail.com\n",
       "Mobile: +91-6303774781\n",
       "LinkedIn: @Ram Badana\n",
       "Hugging Face: Ram HF profile\n",
       "GitHub: Ram580\n",
       "\n",
       "Summary\n",
       "Highly motivated and results-oriented Data Analyst with 2.5 years of experience driving data-driven decision-making through advanced analytics, ML, Deep learning, NLP, and Generative AI solutions. Proven ability to design, develop, and deploy effective ETL pipelines, build and operationalize predictive models, and translate complex data insights into actionable strategies. Proficient in optimizing processes and leveraging advanced ML techniques to consistently deliver impactful results. Adept at various domains – (Pharma, Healthcare, Airlines), quickly adaptable to new technologies.\n",
       "\n",
       "Skills\n",
       "- Programming Language & Packages: Python (Tensorflow, keras, Pytorch, Transformers, Sklearn, NLTK), SQL, PySpark\n",
       "- Data Science: Machine Learning (Regression, Classification, Clustering), Deep Learning (ANN, CNN, Sequence models)\n",
       "- Generative AI: LLMs, Finetuning using PEFT Techniques (LORA, QLORA), RLHF, Multimodal RAG applications, Multi-Agent AI system, RAG Evaluation Techniques (RAGAS), Vector DBs, NoSQL DBs, Langchain – (chains, Tools, Agents), Llama Index, LLMOPS, Hugging Face\n",
       "- Technology & Cloud Platform: AWS, Amazon Bedrock, AWS Sagemaker, Data bricks(beginner), Docker, Github Actions, FastAPI, Flask Rest API, Git\n",
       "\n",
       "Education\n",
       "Manipal University 2018 – 2022\n",
       "B. Tech Automotive Engineering (Minor specialization in EV and Hybrid vehicle Technology)\n",
       "- CGPA: 8.2\n",
       "\n",
       "Accomplishments\n",
       "- 2 X Ambition and Hunger Award – TheMathCompany.\n",
       "- 3 X Appreciation Awards from Multiple Project clients at Mathco.\n",
       "- IBM certified Data scientist.\n",
       "- Secured 2nd place in Mathco Hackathon\n",
       "\n",
       "Certifications\n",
       "- IBM Data science professional Certification.\n",
       "- Deep learning Specialization by Deeplearning.AI.\n",
       "- AWS Generative AI Using LLMs Certification.\n",
       "- Azure Power BI developer.\n",
       "\n",
       "Experience\n",
       "Data Analyst - TheMathCompany, Bengaluru, India\n",
       "June 2022 – Current\n",
       "- Developed a comprehensive Power BI dashboard suite leveraging Power BI, SQL,\n",
       "\n",
       "DriveAI: Senior Software Engineer (remote) — 2015 - 2016\n",
       "- Developed and optimized a central API that significantly improved the functionality used by a large engineering team and thousands of users, enhancing overall system performance and user satisfaction.\n",
       "- Implemented several critical enhancements, including advanced caching strategies that drastically reduced response times and system loads.\n",
       "\n",
       "BetCraft: CTO — 2013 - 2015\n",
       "- Led the technology strategy post-Series A funding, directly reporting to the board and guiding the company through a phase of significant technological advancement and network expansion.\n",
       "- His strategic initiatives and partnerships significantly improved platform performance and expanded the company's market reach.\n",
       "- Helped build his initial product using both React and Angular and got pretty good at it.\n",
       "\n",
       "Education\n",
       "MBA in Information Technology\n",
       "London Business School - MBA\n",
       "\n",
       "Advanced Leadership Techniques\n",
       "University of London - Certification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"./tailored_resume.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dislplay the generated `interview_materials.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the resume and job requirements, here are some potential interview questions and talking points for the candidate:\n",
       "1. Can you elaborate on your experience with Generative AI, particularly in relation to LLMs and other techniques mentioned in your resume?\n",
       "2. How have you collaborated with engineers and scientists to design and develop innovative AI solutions in your previous roles?\n",
       "3. Could you provide an example of a project where you led the implementation of an AI solution and the challenges you faced?\n",
       "4. How do you stay updated with the latest AI technologies and trends, and how do you apply them in your work?\n",
       "5. Describe a situation where you had to troubleshoot technical issues related to AI products and solutions. How did you approach and resolve the problem?\n",
       "6. How do you ensure the performance of AI products and solutions is monitored effectively and improvements are recommended?\n",
       "7. Can you discuss your experience in developing technical documentation, such as user manuals and training materials, for AI solutions?\n",
       "8. How have you leveraged your skills in Machine Learning, Deep Learning, and Generative AI to optimize processes and deliver impactful results in your previous projects?\n",
       "9. What experience do you have with cloud platforms like AWS and technologies like Docker and GitHub Actions in the context of AI development?\n",
       "10. How do you approach working in a team environment, especially when collaborating with engineers and scientists from diverse backgrounds?\n",
       "\n",
       "These questions and talking points can help the candidate showcase their expertise in Generative AI and demonstrate how their skills align with the job requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"./interview_materials.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATULATIONS!!!\n",
    "\n",
    "## Share your accomplishment!\n",
    "- Once you finish watching all the videos, you will see the \"In progress\" image on the bottom left turn into \"Accomplished\".\n",
    "- Click on \"Accomplished\" to view the course completion page with your name on it.\n",
    "- Take a screenshot and share on LinkedIn, X (Twitter), or Facebook.  \n",
    "- **Tag @Joāo (Joe) Moura, @crewAI, and @DeepLearning.AI, (and a few of your friends if you'd like them to try out the course)**\n",
    "- **Joāo and DeepLearning.AI will \"like\"/reshare/comment on your post!**\n",
    "\n",
    "## Get a completion badge that you can add to your LinkedIn profile!\n",
    "- Go to [learn.crewai.com](https://learn.crewai.com).\n",
    "- Upload your screenshot of your course completion page.\n",
    "- You'll get a badge from CrewAI that you can share!\n",
    "\n",
    "(Joāo will also talk about this in the last video of the course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
